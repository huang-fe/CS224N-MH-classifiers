# -*- coding: utf-8 -*-
"""MH classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6zniu7xvR0qzaW8ogmyTTg406CujvgS
"""

# Commented out IPython magic to ensure Python compatibility.
# Install Pytorch & other libraries
%pip install "torch==2.5.1" tensorboard
%pip install flash-attn "setuptools<71.0.0" scikit-learn

!pip install transformers
!pip install datasets
!pip install tokenizers
!pip install accelerate
!pip install hf-transfer

from huggingface_hub import login
from datasets import load_dataset

# Dataset id from huggingface.co/dataset
dataset_id = "youralien/feedback_qesconv_16wayclassification"

# Load raw dataset
raw_dataset = load_dataset(dataset_id, split="train") # happens to be called train

print(f"Raw dataset size: {len(raw_dataset)}")

split_dataset = raw_dataset.train_test_split(test_size=0.2) # 80-20 split
print(f"Train dataset size: {len(split_dataset['train'])}")
print(f"Test dataset size: {len(split_dataset['test'])}")
split_dataset['train'][0]

def prepare_input_text(example):
    # Convert the last two items of input list to a single text
    return {
        'text': "\n".join(example['input'][-2:]),
        **{k:v for k,v in example.items() if k != 'input'}  # Keep other fields
    }

# Apply the preprocessing
split_dataset = split_dataset.map(prepare_input_text)
split_dataset['train'][0]

from transformers import AutoTokenizer

# Model id to load the tokenizer
model_id = "meta-llama/Llama-2-13b-hf"
# model_id = "meta-llama/Llama-3.1-8b"

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.model_max_length = 512 # set model_max_length to 512 as prompts are not longer than 1024 tokens
tokenizer.pad_token = tokenizer.eos_token #??
#tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Tokenize helper

# Tokenize helper function
def tokenize(batch):
    # return tokenizer(batch['text'], padding=True, truncation=True, return_tensors="pt")
    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors="pt")


which_class = "Empathy-goodareas"
#which_class = "Questions-badareas" # "Suggestions-badareas" # "Reflections-badareas" # "Empathy-badareas"
SKILL_OPTIONS = ["Reflections", "Validation", "Empathy", "Questions", "Suggestions", "Self-disclosure", "Structure", "Professionalism"]
goodareas_to_ignore = [f"{skill}-goodareas" for skill in SKILL_OPTIONS if f"{skill}-goodareas" != which_class]
badareas_to_ignore = [f"{skill}-badareas" for skill in SKILL_OPTIONS if f"{skill}-badareas" != which_class]
cols_to_remove = ['conv_index', 'helper_index', 'input', 'text']
cols_to_remove.extend(goodareas_to_ignore)
cols_to_remove.extend(badareas_to_ignore)
if which_class in split_dataset["train"].features.keys():
    split_dataset =  split_dataset.rename_column(which_class, "labels") # to match Trainer
tokenized_dataset = split_dataset.map(tokenize, batched=True, remove_columns=cols_to_remove)

tokenized_dataset["train"].features.keys()
# dict_keys(['labels', 'input_ids', 'attention_mask'])

from transformers import AutoModelForSequenceClassification

# Prepare model labels - useful for inference
labels = ["not selected", "selected"]
num_labels = len(labels)
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label

# Download the model from huggingface.co/models
model = AutoModelForSequenceClassification.from_pretrained(
    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label, device_map="auto")

import numpy as np
from sklearn.metrics import f1_score

# Metric helper method
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    score = f1_score(
            labels, predictions, labels=labels, pos_label=1, average="weighted"
        )
    return {"f1": float(score) if score == 1 else score}

from huggingface_hub import HfFolder
from transformers import Trainer, TrainingArguments

# Define training args
training_args = TrainingArguments(
    output_dir= f"Llama2-{which_class}-classifier",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=16,
    learning_rate=5e-5,
    num_train_epochs=5,
    bf16=True, # bfloat16 training
    optim="adamw_torch_fused", # improved optimizer
    # logging & evaluation strategies
    logging_strategy="steps",
    logging_steps=100,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    # use_mps_device=True, # mps device is a mac thing
    metric_for_best_model="f1",
    # push to hub parameters
    report_to="tensorboard",
    push_to_hub=True,
    hub_strategy="every_save",
    hub_token=HfFolder.get_token(),
)

# Create a Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)
trainer.train()
# {'train_runtime': 3642.7783, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.04, 'train_loss': 0.535627057634551, 'epoch': 5.0}

# Save processor and create model card
tokenizer.save_pretrained(f"Llama2-{which_class}-classifier")
trainer.create_model_card()
trainer.push_to_hub()

import pandas as pd

# condition = "control"
condition = "treatment"
input_data = pd.read_csv(f"../Empathy-Mental-Health/dataset/all_{condition}_seekerhelper_pairs.csv")
input_data.head()

from transformers import pipeline

# load model from huggingface.co/models using our repository id
classifier = pipeline("sentiment-analysis", model=f"Llama2-{which_class}-classifier", device=0)

sample = f"Seeker: {input_data.loc[0, "seeker_post"]}\nHelper: {input_data.loc[0, "response_post"]}"
pred = classifier(sample)
print(pred)

def binary_prediction_seeker_response_post(seeker, helper):
    sample = f"Seeker: {seeker}\nHelper: {helper}"
    pred = classifier(sample)
    return int(pred[0]['label'] == 'selected')

# output_preds = input_data.apply(binary_prediction_seeker_response_post, axis=0)

strengths = [binary_prediction_seeker_response_post(input_data.loc[i, "seeker_post"], input_data.loc[i, "response_post"])
             for i in range(len(input_data))]

input_data[f"{which_class}"] = strengths

input_data.head()

input_data.to_csv(f'all_{condition}_seekerhelper_pairs_{which_class}.csv')

